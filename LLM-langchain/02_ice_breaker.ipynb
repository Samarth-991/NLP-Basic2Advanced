{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ice breaker \n",
    "\n",
    "we are going to connect to Linkdn and fetch information of that person and then will describe and write a summary fo that. We use proxycurl API call to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import json \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\" \n",
    "os.environ['LINKDLN_API_KEY'] = 'oIZkc9gjRGm43huug6xeQg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oIZkc9gjRGm43huug6xeQg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('LINKDLN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_linkdln_data(profile_name = 'samarthtandon991'):\n",
    "    api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "    header_dic = {'Authorization': 'Bearer ' + os.environ.get('LINKDLN_API_KEY')}\n",
    "    params = {\n",
    "        'url': 'https://www.linkedin.com/in/{}/'.format(profile_name),\n",
    "        'fallback_to_cache': 'on-error',\n",
    "        'use_cache': 'if-present',\n",
    "        'skills': 'include',\n",
    "        'inferred_salary': 'include',\n",
    "        'personal_email': 'include',\n",
    "        'personal_contact_number': 'include',\n",
    "        'twitter_profile_id': 'include',\n",
    "        'facebook_profile_id': 'include',\n",
    "        'github_profile_id': 'include',\n",
    "        'extra': 'include',\n",
    "    }\n",
    "\n",
    "    ## respose is saved as Json \n",
    "    response = requests.get(api_endpoint,params=params,headers=header_dic)\n",
    "    return response.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managig tokens:\n",
    "\n",
    "Please read the article https://medium.com/@russkohn/mastering-ai-token-limits-and-memory-ce920630349a \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('../NLP-Basic2Advanced/llm_langchain/sam_profile.json'):\n",
    "    with open('../NLP-Basic2Advanced/llm_langchain/sam_profile.json' ,'r') as fd:  ## Save respose as json\n",
    "        data =json.load(fd)\n",
    "    fd.close()\n",
    "\n",
    "def clean_data(data):\n",
    "    data = {\n",
    "        k:v for k ,v in data.items() \n",
    "        if v not in ([],\"\",\"\",None) and k not in ['people_also_viewed',\"certifications\"]\n",
    "    }\n",
    "    if data.get(\"groups\"):\n",
    "        for group_dict in data.get(\"groups\"):\n",
    "            group_dict.pop('profile_pic_url')\n",
    "\n",
    "    return data\n",
    "\n",
    "clean_data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['experiences'][0]['description'] = \"At TomTom we work on improving maps and navigation using Sate of the Art AI techniques to provide better map experience to our customers.As a senior data scientist I work on aerial imagenary to extract building footprints data to improve address and Positional accuracy . With This we also help our automobiles customers to accurately pinpoint routing points vis parking lots and parking exsits for better navigation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_name = clean_data['full_name']\n",
    "country = clean_data['country_full_name']\n",
    "profile_description = []\n",
    "\n",
    "for d in clean_data['experiences']:\n",
    "    company_name = d['company']\n",
    "    if d['ends_at'] is None:\n",
    "        profile_description.append(\"{} from {}is currently working in {} as {}. He describes his work at {} as: {}\".format(profile_name,country\n",
    "                                                                                                                           ,company_name,d['title'],\n",
    "                                                                                              company_name,d['description']))\n",
    "    else:\n",
    "        period = (d['starts_at']['year'], d['ends_at']['year'])\n",
    "        profile_description.append(\"{} also has worked at {} as {} from {} to {}. He describes his work at {} as {}\".format(\n",
    "            profile_name,company_name,d['title'],period[0],period[1],company_name,d['description']))\n",
    "        \n",
    "profile_description = ''.join(profile_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM model : gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "# prompt \n",
    "summary_template = \"\"\" given the information {information} about  a person from I want you to create:\n",
    "                        1. A short summary\n",
    "                        2. Two interesting facts about the person \n",
    "                        3. Highlight the key skills of person \n",
    "                    \"\"\"\n",
    "summary_prompt_template = PromptTemplate(template=summary_template,input_variables=[\"information\"])\n",
    "# input var is information we run the cahin keyword is information is valid.\n",
    "llm = ChatOpenAI(temperature=1,model_name=\"gpt-3.5-turbo\")\n",
    "print(\"LLM model :\",llm.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Samarth Tandon is a Senior Data Scientist from India currently working at TomTom. He has also worked at Faurecia and NVIDIA in various roles related to AI development for autonomous driving vehicles and manufacturing. He has experience in developing deep learning algorithms for facial recognition, building footprints extraction, and automated quality assessment in manufacturing.\n",
      "\n",
      "2. Interesting facts about Samarth Tandon: \n",
      "- He has worked on improving the accuracy of address and positional data for TomTom maps using aerial imagery.\n",
      "- Samarth has also developed AI tools for testing autonomous driving vehicles in different conditions using image-to-image translation techniques.\n",
      "\n",
      "3. Key skills of Samarth Tandon:\n",
      "- Advanced skills in computer vision and deep learning algorithms\n",
      "- Expertise in using various deep learning frameworks like Theano, TensorFlow, and Keras\n",
      "- Experience in developing AI solutions for manufacturing and autonomous driving domains\n",
      "- Strong programming skills in Python, C++, and MATLAB\n",
      "- Experience in managing teams and leading end-to-end development and deployment of AI models and pipelines.\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=summary_prompt_template)\n",
    "print(chain.run(information=profile_description))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents \n",
    "Till now we have not used the power of the Langchain yet , just mere a API call to fetch data and play with it to create summary.\n",
    "\n",
    "- Agents are type of chains which has access to multiple tools . Depending on  user input it can be decided to call any tool.\n",
    "- Agents are kind of a bot that will perform certain actions on behlaf of you and its able to interact with LMS.\n",
    "\n",
    "So to simplify, It connects the long chain framework with those third parties, with those external services.\n",
    "\n",
    "So, so once we give the task to the agent, the first thing he does it calculates what subtasks you need to perform in order to achieve and to complete the big task that we sent it.\n",
    "\n",
    "Now to break down to subtasks, is something that is totally capable by the LLMS.If we need to access something on which LLM is not trained , Agents solve that problem.\n",
    "\n",
    "### Chain of Thought \n",
    "\n",
    "Chain of thought, basically it's a prompt engineering technique which allows the LM to reason and to show us its reasoning.Paper : chain of thought prompting Elicits Reasoning in LLM\n",
    "\n",
    "### REACT  \n",
    "Once it lists it subtasks , it has the ability to perform thoses subtasks and retruning the answers we want.Paper : Synergizing Reasoning and acting in LLM's, which is also a prompt engineering technique.This is how the LangChain is built under the hood.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective : Linkdln Lookup Agent\n",
    "We want to give a name and ask  the Agen to find the Linkdln URL for that person "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import initialize_agent , Tool\n",
    "from langchain.agents import AgentType # tells Agent how to calculate the sub task we need to complete \n",
    "from langchain.serpapi import SerpAPIWrapper\n",
    "\n",
    "from tenacity import retry ,wait_random\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = '836b6623bddd5b44cf8d3e01bd80f262ec123f364591b1e4a35d736b9d7237d0'\n",
    "\n",
    "#since the SERP API is not looking where it should be looking at we need to write a custom wrapper\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "class Custom_SerpAPIWrapper(SerpAPIWrapper):\n",
    "    def __init__(self):\n",
    "        super(Custom_SerpAPIWrapper,self).__init__()\n",
    "    @staticmethod\n",
    "    def _process_response(res: dict) -> str:\n",
    "        \n",
    "        # process response from SerpAPI \n",
    "        if 'error' in res.keys():\n",
    "            raise ValueError('got error from SerpAPI')\n",
    "        \n",
    "        if \"answer_box\" in res.keys() and \"answer\" in res[\"answer_box\"].keys():\n",
    "            toret = res[\"answer_box\"][\"answer\"]\n",
    "        \n",
    "        elif \"answer_box\" in res.keys() and \"snippet\" in res[\"answer_box\"].keys():\n",
    "            toret = res[\"answer_box\"][\"snippet\"]\n",
    "        \n",
    "        elif (\n",
    "            \"answer_box\" in res.keys()\n",
    "            and \"snippet_highlighted_words\" in res[\"answer_box\"].keys()\n",
    "        ):\n",
    "            toret = res[\"answer_box\"][\"snippet_highlighted_words\"][0]\n",
    "        \n",
    "        elif (\n",
    "            \"sports_results\" in res.keys()\n",
    "            and \"game_spotlight\" in res[\"sports_results\"].keys()\n",
    "        ):\n",
    "            toret = res[\"sports_results\"][\"game_spotlight\"]\n",
    "        \n",
    "        elif (\n",
    "            \"knowledge_graph\" in res.keys()\n",
    "            and \"description\" in res[\"knowledge_graph\"].keys()\n",
    "        ):\n",
    "            toret = res[\"knowledge_graph\"][\"description\"]\n",
    "        \n",
    "        # elif \"title\" in res[\"organic_results\"][0].keys():\n",
    "        #     toret = res[\"organic_results\"][0][\"title\"]\n",
    "\n",
    "        elif \"snippet\" in res[\"organic_results\"][0].keys():\n",
    "            toret = res[\"organic_results\"][0][\"link\"]\n",
    "        else:\n",
    "            toret = \"No good search result found\"\n",
    "        return toret\n",
    "\n",
    "\n",
    "def get_profile_url(text:str):\n",
    "    \"\"\" searches for linkdin page\"\"\"\n",
    "    search = Custom_SerpAPIWrapper()\n",
    "    result = search.run(f\"{text}\")\n",
    "    return result \n",
    "\n",
    "\n",
    "@retry(wait=wait_random(min=2, max=5))\n",
    "def lookup(name:str):\n",
    "    llm = ChatOpenAI(temperature=0,model_name='gpt-3.5-turbo')\n",
    "    template = \"\"\"\n",
    "                Given the full name {name_of_person} I want you to get it me the link of linkedin profile page:\n",
    "                Your answer will contain only the linkedIn URL  \n",
    "                \"\"\"\n",
    "    tools_for_agent = [\n",
    "        Tool(name=\"Crawl Google 4 linkedin profile URL\",\n",
    "             func=get_profile_url,\n",
    "             description='useful when you need to get linkedin profile Page URL') # agent uses descrption to decide which tool to use\n",
    "             ] \n",
    "    agent = initialize_agent(tools=tools_for_agent,llm=llm,\n",
    "                             agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True) # with verbose we see the resoning process\n",
    "    prompt_template = PromptTemplate(input_variables=['name_of_person'],template=template)\n",
    "    \n",
    "    linkdln_profile_url = agent.run(prompt_template.format_prompt(name_of_person=name))\n",
    "    \n",
    "    return linkdln_profile_url"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents use LLM to determine which action need to be taken and in which order need to be taken\n",
    "\n",
    "- Agent type is a super important parameter because it will decide what is the reasoning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_linkdln_profile(profile_url):\n",
    "    api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "    header_dic = {'Authorization': 'Bearer ' + os.environ.get('LINKDLN_API_KEY')}\n",
    "    params = {\n",
    "        'url': '{}'.format(profile_url),\n",
    "        'fallback_to_cache': 'on-error',\n",
    "        'use_cache': 'if-present',\n",
    "        'skills': 'include',\n",
    "        'inferred_salary': 'include',\n",
    "        'personal_email': 'include',\n",
    "        'personal_contact_number': 'include',\n",
    "        'twitter_profile_id': 'include',\n",
    "        'facebook_profile_id': 'include',\n",
    "        'github_profile_id': 'include',\n",
    "        'extra': 'include',\n",
    "    }\n",
    "    ## respose is saved as Json \n",
    "    response = requests.get(api_endpoint,params=params,headers=header_dic)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_url = lookup('Eden Marco Udemy')\n",
    "response = scrape_linkdln_profile(profile_url)\n",
    "response.content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
