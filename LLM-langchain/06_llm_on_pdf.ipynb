{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synergizing Reasoning and Acting in LLM\n",
    "\n",
    "Language models are getting better at reasoning (e.g. chain-of-thought prompting) and acting (e.g. WebGPT, SayCan, ACT-1), but these two directions have remained separate.\n",
    "ReAct asks, what if these two fundamental capabilities are combined?\n",
    "\n",
    "For more information please read the documentation in link [model_with_Code](https://react-lm.github.io/)\n",
    "\n",
    "### Pipeline \n",
    "\n",
    "![pipeline](https://user-images.githubusercontent.com/1031925/217168553-d74ef962-1a9d-4351-8c96-9033e65d58ab.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf\n",
    "import os\n",
    "\n",
    "from  langchain.document_loaders import PyPDFLoader  # https://python.langchain.com/en/latest/modules/indexes/document_loaders.html\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "import openai\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\" \n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "pdf_path = '../NLP-Basic2Advanced/data/synergizing_reasoning_and_acting_LLM.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=pdf_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the document 33\n"
     ]
    }
   ],
   "source": [
    "print(\"Total length of the document {}\".format(len(documents)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control the doc size to not hit the Token limit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1038, which is longer than the specified 1000\n",
      "Created a chunk of size 2273, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=30,separator='\\n')\n",
    "docs = text_splitter.split_documents(documents=documents)  # metadatas=[doc.metadata for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Docs after text splitter 131\n",
      "showing 10 such docs :\n",
      " \n",
      "total number of words in PDF 110203\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of Docs after text splitter {}\".format(len(docs)))\n",
    "print(\"showing 10 such docs :\\n \")\n",
    "\n",
    "count = 0\n",
    "for doc in docs:\n",
    "    count+= len(doc.page_content)\n",
    "\n",
    "print(\"Total number of words in PDF: {}\".format(count))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-embedding-ada-002\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "openai_embeddings = OpenAIEmbeddings()\n",
    "print(openai_embeddings.model)\n",
    "print(openai_embeddings.chunk_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Hugging face Embedddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samartht/anaconda3/envs/pylangchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class LocalHuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id): \n",
    "        # Should use the GPU by default\n",
    "        self.model = SentenceTransformer(model_id)\n",
    "        \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents using a locally running\n",
    "           Hugging Face Sentence Transformer model\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        embeddings =self.model.encode(texts)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a query using a locally running HF \n",
    "        Sentence trnsformer. \n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return list(map(float, embedding))\n",
    "\n",
    "local_embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS VectorDatabase\n",
    "\n",
    "- FAISS takes very big vectors, then it will be able to convert them into objects that are small enough to be saved in the RAM.\n",
    "- it's not a managed robust vector database like Pinecone, which handles scalability, availability,durability but it runs locally \n",
    "\n",
    "![FAISS_working](https://engineering.fb.com/wp-content/uploads/2017/03/GOcmDQEFmV52jukHAAAAAAAqO6pvbj0JAAAB.jpg)\n",
    "\n",
    "[Read blog](https://medium.com/dotstar/understanding-faiss-part-2-79d90b1e5388)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore =  FAISS.from_documents(documents=docs,embedding = local_embeddings) # turn dcos into Vectors and store them in RAM\n",
    "vectorstore.save_local('../NLP-Basic2Advanced/llm_langchain/faiss_index_react')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrival QA\n",
    "Better version of Query \n",
    "\n",
    "###  underneath the hood \n",
    "- Underneath the hood is taking our query that we're going which is send to embed.\n",
    "- Its turned into a vector, send it into device vector store.\n",
    "- Then it's going to find similar vectors to that vector and it's going to bring those vectors back.\n",
    "- Translate them into the text and we're going to send this as the context exactly like the vector chain, but with a bit of different parameters, but nothing very different here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "new_vector_store = FAISS.load_local(\"../NLP-Basic2Advanced/llm_langchain/faiss_index_react/\",embeddings=local_embeddings)\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(),chain_type='stuff',retriever=new_vector_store.as_retriever())  # stuff means once we get the vectors after similiarity search to use in our context to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qa.run(\"give me the gist of ReAct in three sentences point wise \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ReAct is an agent-based system that augments action spaces to include language; it enables interpretable, sequential decision-making and reasoning processes that can be easily inspected and corrected; and it demonstrates synergy between reasoning and acting to perform knowledge-intensive tasks.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets  Use Open AI Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore =  FAISS.from_documents(documents=docs,embedding = openai_embeddings) # turn dcos into Vectors and store them in RAM\n",
    "vectorstore.save_local('../NLP-Basic2Advanced/llm_langchain/faiss_index_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ReAct is an agent-augmented action space that allows agents to reason over current context and update contexts for future actions. It enables humans to control or correct agents on the go by thought editing. It allows reasoning and acting to be synergistic, allowing agents to interact with external APIs.\n"
     ]
    }
   ],
   "source": [
    "new_vector_store = FAISS.load_local(\"../NLP-Basic2Advanced/llm_langchain/faiss_index_react/\",embeddings=local_embeddings)\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(),chain_type='stuff',retriever=new_vector_store.as_retriever())\n",
    "\n",
    "res = qa.run(\"give me the gist of ReAct in three sentences point wise \")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
