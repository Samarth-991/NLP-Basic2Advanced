{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samartht/anaconda3/envs/pylangchain/lib/python3.10/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from  langchain.document_loaders import TextLoader  # https://python.langchain.com/en/latest/modules/indexes/document_loaders.html\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.vectorstores.redis import Redis as RedisVectorStore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain import VectorDBQA,OpenAI \n",
    "import pinecone\n",
    "import openai\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\" \n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Loader \n",
    "- Handles documents of any type to digest in best possible way \n",
    "\n",
    "#### Text Splitter \n",
    "- takes large text and split into multiple chunks . Char text splitter helps to select the text and overlap parameter help us to keep useful data\n",
    "\n",
    "#### OpenAI embeddings \n",
    "- Text to Vector data text --> embed Model --> Vector sapaces\n",
    "\n",
    "### VectorDBQA \n",
    "- It retrieves relevant documents from the vector database based on the input query and then uses the LLM to generate an answer.\n",
    "\n",
    "All that work of getting the query, embedding it, sending it to the vector databases, then looking for similar vectors, getting those vectors, retrieving them back,then turning them into the texts and chunks, and then plugging all this in into the prompt to ask the\n",
    "LM Wow, all that work simply in one command.\n",
    "\n",
    "Entire Pipleine Looks like :\n",
    "\n",
    "![pipeline](https://images.ctfassets.net/xjan103pcp94/4OzISThpksdKgjZ0gVJUiB/85bb7fccdfef1df3d061c57e9af1062a/index-langchain.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the PineCone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinecone.init(\n",
    "#     api_key=os.environ[\"304950dd-a5e0-406a-9c7f-d47602abcfab\"],\n",
    "#     environment=os.environ[\"northamerica-northeast1-gcp\"],\n",
    "# )\n",
    "# INDEX_NAME = \"langchain-doc-index\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize te Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import redis\n",
    "\n",
    "# os.environ['REDIS_APIKEY'] = 'Sre0wacy60g8owb7k6o2glkdat3nakpgcucy4daw172892pbkv'\n",
    "# os.environ['REDIS_PASSWORD'] = '1h5tBoYxZbmX1oAdSCKZxxGsfIQ81kF9'\n",
    "# os.environ['REDIS_HOST'] = 'redis-10650.c282.east-us-mz.azure.cloud.redislabs.com'\n",
    "# os.environ['REDIS_URL'] = 'redis://127.0.0.1:8000'\n",
    "\n",
    "# r = redis.Redis(\n",
    "#   host='redis-10650.c282.east-us-mz.azure.cloud.redislabs.com',\n",
    "#   port=8000,\n",
    "#   password=os.getenv('REDIS_PASSWORD'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document [Document(page_content='Word embedding is a technique used in natural language processing (NLP) to map words or phrases from a vocabulary to a continuous vector space. The goal of word embedding is to capture the meaning of a word in a vector representation, such that similar words have similar representations and dissimilar words have dissimilar representations. There are several word embedding techniques, but the most popular ones are:\\n\\nWord2Vec: It is a neural network-based technique that uses a shallow neural network to learn the vector representations of words. Word2Vec has two algorithms: Continuous Bag of Words (CBOW) and Skip-Gram.\\nGloVe: This method stands for “Global Vectors for Word Representation” It combines the advantages of the matrix factorization techniques and the global corpus statistics.\\nFastText: It is an extension of the word2vec model that learns to take into account subword information (i.e. n-grams) of a word in addition to the word itself. It’s particularly useful for handling rare or out-of-vocabulary words.\\nELMO: This is deep contextualized word representations which is pre-trained models which are task-agnostic, giving it more context while learning word representation.\\nBERT: Like ELMO, BERT is also a deep contextualized word representation. BERT is trained on a massive amount of data and can be fine-tuned on a variety of tasks.\\nThese word embedding techniques are widely used in various natural language processing tasks, such as text classification, text generation, machine translation, named entity recognition, and more.\\n\\nA word embedding is a representation of a word as a vector of numeric values.\\n\\nFor example, the word “night” might be represented as (-0.076, 0.031, -0.024, 0.022, 0.035).\\n\\nIt means that if there are two words used similarly in the text, they will have the same vector representations.\\n\\nAfter mapping words into the vector space, we can use the vector math to find words with similar semantics.\\n\\nWe can either create a custom word embedding based on our project or use pre-built word embeddings by loading them.\\n\\nTo find the word embedding of text, first we need to split it to sentence and then tokenize the sentences.\\n•Then we import Word2Vec model from genism and train it on our text.\\n\\n•There are many parameters that we can specify for our Word2Vec model.\\n\\n“size” is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\\n\\nBigger size values require more training data, but can lead to better (more accurate) models.\\n\\n“min_count” is the minimum number of occurrence of a word to be considered in the model.\\n\\nWe can check the vector representation of a word in the vocabulary.\\n\\n\\nWhy Word Embedding is required\\n\\nwe could represent words as single integer values, for example, “hello” = 1, “world” = 2, “cat” = 3, and so on.\\n\\nThere are two main problems in representing words with integers:\\n\\nFirst, because of the way neural networks compute their output, words that are numerically close are indirectly processed as being semantically close.\\n\\nIn the example above, “world” and “cat” would be processed as if they were very similar in some way\\n\\nSecond, even if we could represent words that are close together in meaning with numeric values that are close, you could only represent similarity in one dimension.\\n\\nthe words “man” and “boy” can be compared on the dimension of gender (the same), or on age (a bit different), or on likelihood of being a software engineer (much different)\\n\\nWord2Vec Algorithm\\n\\nWord2Vec is a word embedding algorithm developed by Google researchers in 2013. It is used to map words or phrases from a vocabulary to a continuous vector space. The goal of Word2Vec is to capture the meaning of a word in a vector representation, such that similar words have similar representations and dissimilar words have dissimilar representations.\\n\\nWord2Vec has two main algorithms for learning the vector representations of words:\\n\\nContinuous Bag of Words (CBOW): This algorithm uses the context of a word to predict the word itself. For example, given the context “the cat sat on”, the algorithm would try to predict the word “mat”. The vector representations of words are learned by training a neural network on the task of predicting the current word based on its surrounding context.\\nSkip-Gram: This algorithm is the opposite of CBOW, it tries to predict the context of a given word. The vector representations of words are learned by training a neural network on the task of predicting the surrounding context based on the current word.\\nBoth CBOW and Skip-gram algorithms uses a shallow neural network with one hidden layer, called an embedding layer, that learns the vector representations of words. The input to the network is the one-hot encoded representation of a word, and the output is the predicted context or word. The learned vector representations are stored in the embedding layer and can be used for various NLP tasks.\\n\\nThe main difference between these two algorithms is that the CBOW algorithm is faster and the Skip-Gram algorithm is more accurate. The choice of algorithm will depend on the specific requirements of your project.\\n\\nNeural word embedding\\n\\nNeural word embeddings are a type of word embedding technique that uses neural networks to learn the vector representations of words. These vector representations, also known as embeddings, capture the meaning of a word in a continuous vector space, such that similar words have similar representations and dissimilar words have dissimilar representations.\\n\\nStatic word embedding\\n\\nStatic word embeddings, also known as pre-trained word embeddings, are a type of word embedding technique where a fixed set of vector representations of words are pre-computed and stored in a lookup table. These vector representations are learned from a large corpus of text using a word embedding algorithm, such as Word2Vec or GloVe, and then used as an input to other natural language processing (NLP) tasks.\\n\\nUnlike other dynamic word embeddings, where the embeddings are learned as part of a larger neural network model and updated during the training process, static word embeddings are fixed and cannot be updated. Instead, they are used as a lookup table to provide pre-trained embeddings to the model.\\n\\nContextualized word embedding\\n\\nContextualized word embeddings are a type of word embedding technique that takes into account the context in which a word appears in order to provide a more accurate representation of the word’s meaning.\\n\\nTraditional word embeddings like word2vec or GloVe represent each word with a fixed, pre-trained vector, which is used across all contexts in which the word appears. However, the meaning of a word can depend on the context in which it appears. For example, the word “apple” could refer to the fruit or the company, depending on the context.\\n\\nContextualized word embeddings aim to overcome this limitation by generating a new vector representation for each word in its specific context. This is achieved by fine-tuning pre-trained models on a specific task or dataset and the models like ELMO, BERT are trained on a massive amount of data which allows them to take into account the context in which a word appears and learn a more accurate representation of the word’s meaning.\\n\\nFor example, BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based model that can learn contextualized word representations by considering the surrounding text in both directions. In this way, it can generate different embeddings for the same word depending on its context, allowing for a more accurate understanding of the word’s meaning in different situations.\\n\\nContextualized word embeddings have been shown to improve performance on a wide range of NLP tasks such as named entity recognition, question answering, sentiment analysis, etc. and they have become the state-of-the-art method for many tasks in recent years.\\n\\nAdvances in Natural Language processing\\n\\nELMO\\n\\nELMO (Embeddings from Language Models) is a deep contextualized word representation technique developed by researchers at the Allen Institute for Artificial Intelligence. It is pre-trained model which is task-agnostic, meaning it can be fine-tuned on a wide range of natural language processing (NLP) tasks and it uses a deep neural network to learn the vector representations of words.\\n\\nThe ELMO model is trained on a massive corpus of text and uses a bi-directional LSTM (Long Short-term Memory) network architecture, which allows it to take into account the context in which a word appears, and learn a more accurate representation of the word’s meaning.\\n\\nThe main difference between ELMO and other pre-trained models like BERT, word2vec, GloVe is the ELMO model learns a different representation for each word depending on its context within a given sentence.\\n\\nIn other words, the ELMO model generates a unique vector representation of a word for each instance it appears in a sentence, rather than a single fixed representation that’s used across all contexts. This allows for a more nuanced understanding of the word’s meaning, which can improve the performance of NLP tasks like language translation, named entity recognition, text classification, question answering, and more.\\n\\nIn short, ELMO is pre-trained model that provides contextualized word representations, which can be fine-tuned on a variety of NLP tasks and it has been shown to improve the performance on a wide range of NLP tasks.\\n\\nBERT\\n\\nBERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based model for natural language processing (NLP) tasks, developed by researchers at Google. It is designed to understand the context of the words in a sentence, by considering the surrounding text in both directions (left and right). This makes BERT one of the most powerful techniques for creating contextualized word representations.\\n\\nUnlike traditional pre-trained models like word2vec or GloVe, which are trained on a unidirectional context, BERT uses a bidirectional transformer encoder architecture, which allows it to learn the context of a word by considering the words that come before and after it in a sentence.\\n\\nBERT is trained on a massive amount of text data and pre-trained on two unsupervised tasks: masked language modeling and next-sentence prediction. This makes it possible to fine-tune BERT on a wide range of NLP tasks, such as text classification, named entity recognition, question answering, and more, by just adding a small amount of task-specific training data.\\n\\nBERT has shown to perform well on a wide range of NLP tasks and it became one of the state-of-the-art models in several benchmarks datasets. Since its release, many variants of BERT have been developed such as RoBERTa, ALBERT and DistilBERT, with the goal of making it more efficient and widely accessible.\\n\\nGPT\\n\\nGPT (Generative Pre-trained Transformer) is a language model developed by OpenAI, which is trained to generate natural language text. It is based on the transformer architecture, which is a type of neural network that has been shown to be effective at a variety of natural language processing (NLP) tasks.\\n\\nGPT is pre-trained on a massive corpus of text data, and it uses this training to generate text that is similar to human writing. The model is trained to predict the next word in a sentence, given the context of the previous words. During training, the model learns to understand the meaning of words and their relationships with each other, allowing it to generate coherent and fluent text.\\n\\nGPT can be fine-tuned on a wide range of NLP tasks such as text classification, text generation, text summarization, question answering, etc. The pre-training process allows the model to learn a general understanding of the language, allowing it to be adapted to a specific task with just a small amount of task-specific training data.\\n\\nAdditionally, GPT-3(Chat GPT), a more advanced version of GPT-2, has been developed and with its even more impressive performance on a wide range of NLP tasks. it has the ability to perform well in various NLP tasks without fine-tuning, showing the potential for GPT models to become general-purpose NLP models.\\n\\nGPT-3 (Chat GPT) is a powerful language model that can be used in a wide range of NLP tasks, including chatbot systems, with its ability to generate natural-sounding text based on user input, it could make chatbot systems more human-like and engaging for users.\\n\\n\\nn summary, GPT is a pre-trained model that uses a transformer-based architecture to generate natural language text and it has shown impressive performance on various NLP tasks.\\n\\nWord2Vec use cases\\n\\nWord2Vec is a word embedding algorithm that is used to map words or phrases from a vocabulary to a continuous vector space. It is a powerful tool for natural language processing (NLP) tasks, and has many practical use cases. Some of the most common use cases for Word2Vec are:\\n\\nText classification: Word2Vec can be used to convert text into numerical data, which can then be used as input for a machine learning model to perform text classification tasks such as sentiment analysis or spam detection.\\nWord similarity: One of the main advantages of Word2Vec is that it can learn the similarity of words, it can be used to find synonyms or related words based on the similarity of the learned embeddings.\\nNamed Entity Recognition: The embeddings can be used as features for a model that identifies entities in text such as organizations, person and locations\\nText Generation: The embeddings learned by Word2Vec can be used as input to a language model such as LSTM, to generate text that is similar to the training data.\\nClustering: The embeddings can be used to cluster words or documents into groups, based on their similarity.\\nDimensionality reduction: Word2Vec can be used to reduce the dimensionality of the text data, which can be useful for visualizing the relationships between words or for reducing the computational cost of other NLP tasks.\\nRecommender systems: The embeddings can be used to find similar items (such as products, movies, or articles) based on their similarity in word usage, which can be useful for building recommenders systems\\nMachine Translation: Word2Vec can be used as a pre-processing step for machine translation, where the embeddings of the words in the source language can be used to initialize the embeddings of the words in the target language\\nThese are just a few examples of the many potential use cases for Word2Vec, and it’s likely that new applications will continue to be discovered as the field of NLP evolves.\\n', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'})]\n",
      "Type:<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"../NLP-Basic2Advanced/data/medium_logs_1.txt\")\n",
    "document = loader.load()\n",
    "print(\"Document {}\\nType:{}\".format(document,type(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1133, which is longer than the specified 1024\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1024 , chunk_overlap=20) # fine tune parameters \n",
    "texts = text_splitter.split_documents(documents=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text : 18\n",
      "<class 'list'>\n",
      "[Document(page_content='Word embedding is a technique used in natural language processing (NLP) to map words or phrases from a vocabulary to a continuous vector space. The goal of word embedding is to capture the meaning of a word in a vector representation, such that similar words have similar representations and dissimilar words have dissimilar representations. There are several word embedding techniques, but the most popular ones are:', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='Word2Vec: It is a neural network-based technique that uses a shallow neural network to learn the vector representations of words. Word2Vec has two algorithms: Continuous Bag of Words (CBOW) and Skip-Gram.\\nGloVe: This method stands for “Global Vectors for Word Representation” It combines the advantages of the matrix factorization techniques and the global corpus statistics.\\nFastText: It is an extension of the word2vec model that learns to take into account subword information (i.e. n-grams) of a word in addition to the word itself. It’s particularly useful for handling rare or out-of-vocabulary words.\\nELMO: This is deep contextualized word representations which is pre-trained models which are task-agnostic, giving it more context while learning word representation.\\nBERT: Like ELMO, BERT is also a deep contextualized word representation. BERT is trained on a massive amount of data and can be fine-tuned on a variety of tasks.\\nThese word embedding techniques are widely used in various natural language processing tasks, such as text classification, text generation, machine translation, named entity recognition, and more.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='A word embedding is a representation of a word as a vector of numeric values.\\n\\nFor example, the word “night” might be represented as (-0.076, 0.031, -0.024, 0.022, 0.035).\\n\\nIt means that if there are two words used similarly in the text, they will have the same vector representations.\\n\\nAfter mapping words into the vector space, we can use the vector math to find words with similar semantics.\\n\\nWe can either create a custom word embedding based on our project or use pre-built word embeddings by loading them.\\n\\nTo find the word embedding of text, first we need to split it to sentence and then tokenize the sentences.\\n•Then we import Word2Vec model from genism and train it on our text.\\n\\n•There are many parameters that we can specify for our Word2Vec model.\\n\\n“size” is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\\n\\nBigger size values require more training data, but can lead to better (more accurate) models.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='“min_count” is the minimum number of occurrence of a word to be considered in the model.\\n\\nWe can check the vector representation of a word in the vocabulary.\\n\\n\\nWhy Word Embedding is required\\n\\nwe could represent words as single integer values, for example, “hello” = 1, “world” = 2, “cat” = 3, and so on.\\n\\nThere are two main problems in representing words with integers:\\n\\nFirst, because of the way neural networks compute their output, words that are numerically close are indirectly processed as being semantically close.\\n\\nIn the example above, “world” and “cat” would be processed as if they were very similar in some way\\n\\nSecond, even if we could represent words that are close together in meaning with numeric values that are close, you could only represent similarity in one dimension.\\n\\nthe words “man” and “boy” can be compared on the dimension of gender (the same), or on age (a bit different), or on likelihood of being a software engineer (much different)\\n\\nWord2Vec Algorithm', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='Word2Vec Algorithm\\n\\nWord2Vec is a word embedding algorithm developed by Google researchers in 2013. It is used to map words or phrases from a vocabulary to a continuous vector space. The goal of Word2Vec is to capture the meaning of a word in a vector representation, such that similar words have similar representations and dissimilar words have dissimilar representations.\\n\\nWord2Vec has two main algorithms for learning the vector representations of words:', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='Continuous Bag of Words (CBOW): This algorithm uses the context of a word to predict the word itself. For example, given the context “the cat sat on”, the algorithm would try to predict the word “mat”. The vector representations of words are learned by training a neural network on the task of predicting the current word based on its surrounding context.\\nSkip-Gram: This algorithm is the opposite of CBOW, it tries to predict the context of a given word. The vector representations of words are learned by training a neural network on the task of predicting the surrounding context based on the current word.\\nBoth CBOW and Skip-gram algorithms uses a shallow neural network with one hidden layer, called an embedding layer, that learns the vector representations of words. The input to the network is the one-hot encoded representation of a word, and the output is the predicted context or word. The learned vector representations are stored in the embedding layer and can be used for various NLP tasks.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='The main difference between these two algorithms is that the CBOW algorithm is faster and the Skip-Gram algorithm is more accurate. The choice of algorithm will depend on the specific requirements of your project.\\n\\nNeural word embedding\\n\\nNeural word embeddings are a type of word embedding technique that uses neural networks to learn the vector representations of words. These vector representations, also known as embeddings, capture the meaning of a word in a continuous vector space, such that similar words have similar representations and dissimilar words have dissimilar representations.\\n\\nStatic word embedding', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='Static word embeddings, also known as pre-trained word embeddings, are a type of word embedding technique where a fixed set of vector representations of words are pre-computed and stored in a lookup table. These vector representations are learned from a large corpus of text using a word embedding algorithm, such as Word2Vec or GloVe, and then used as an input to other natural language processing (NLP) tasks.\\n\\nUnlike other dynamic word embeddings, where the embeddings are learned as part of a larger neural network model and updated during the training process, static word embeddings are fixed and cannot be updated. Instead, they are used as a lookup table to provide pre-trained embeddings to the model.\\n\\nContextualized word embedding\\n\\nContextualized word embeddings are a type of word embedding technique that takes into account the context in which a word appears in order to provide a more accurate representation of the word’s meaning.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='Traditional word embeddings like word2vec or GloVe represent each word with a fixed, pre-trained vector, which is used across all contexts in which the word appears. However, the meaning of a word can depend on the context in which it appears. For example, the word “apple” could refer to the fruit or the company, depending on the context.\\n\\nContextualized word embeddings aim to overcome this limitation by generating a new vector representation for each word in its specific context. This is achieved by fine-tuning pre-trained models on a specific task or dataset and the models like ELMO, BERT are trained on a massive amount of data which allows them to take into account the context in which a word appears and learn a more accurate representation of the word’s meaning.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='For example, BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based model that can learn contextualized word representations by considering the surrounding text in both directions. In this way, it can generate different embeddings for the same word depending on its context, allowing for a more accurate understanding of the word’s meaning in different situations.\\n\\nContextualized word embeddings have been shown to improve performance on a wide range of NLP tasks such as named entity recognition, question answering, sentiment analysis, etc. and they have become the state-of-the-art method for many tasks in recent years.\\n\\nAdvances in Natural Language processing\\n\\nELMO', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='ELMO\\n\\nELMO (Embeddings from Language Models) is a deep contextualized word representation technique developed by researchers at the Allen Institute for Artificial Intelligence. It is pre-trained model which is task-agnostic, meaning it can be fine-tuned on a wide range of natural language processing (NLP) tasks and it uses a deep neural network to learn the vector representations of words.\\n\\nThe ELMO model is trained on a massive corpus of text and uses a bi-directional LSTM (Long Short-term Memory) network architecture, which allows it to take into account the context in which a word appears, and learn a more accurate representation of the word’s meaning.\\n\\nThe main difference between ELMO and other pre-trained models like BERT, word2vec, GloVe is the ELMO model learns a different representation for each word depending on its context within a given sentence.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='In other words, the ELMO model generates a unique vector representation of a word for each instance it appears in a sentence, rather than a single fixed representation that’s used across all contexts. This allows for a more nuanced understanding of the word’s meaning, which can improve the performance of NLP tasks like language translation, named entity recognition, text classification, question answering, and more.\\n\\nIn short, ELMO is pre-trained model that provides contextualized word representations, which can be fine-tuned on a variety of NLP tasks and it has been shown to improve the performance on a wide range of NLP tasks.\\n\\nBERT', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='BERT\\n\\nBERT (Bidirectional Encoder Representations from Transformers) is a pre-trained transformer-based model for natural language processing (NLP) tasks, developed by researchers at Google. It is designed to understand the context of the words in a sentence, by considering the surrounding text in both directions (left and right). This makes BERT one of the most powerful techniques for creating contextualized word representations.\\n\\nUnlike traditional pre-trained models like word2vec or GloVe, which are trained on a unidirectional context, BERT uses a bidirectional transformer encoder architecture, which allows it to learn the context of a word by considering the words that come before and after it in a sentence.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='BERT is trained on a massive amount of text data and pre-trained on two unsupervised tasks: masked language modeling and next-sentence prediction. This makes it possible to fine-tune BERT on a wide range of NLP tasks, such as text classification, named entity recognition, question answering, and more, by just adding a small amount of task-specific training data.\\n\\nBERT has shown to perform well on a wide range of NLP tasks and it became one of the state-of-the-art models in several benchmarks datasets. Since its release, many variants of BERT have been developed such as RoBERTa, ALBERT and DistilBERT, with the goal of making it more efficient and widely accessible.\\n\\nGPT\\n\\nGPT (Generative Pre-trained Transformer) is a language model developed by OpenAI, which is trained to generate natural language text. It is based on the transformer architecture, which is a type of neural network that has been shown to be effective at a variety of natural language processing (NLP) tasks.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='GPT is pre-trained on a massive corpus of text data, and it uses this training to generate text that is similar to human writing. The model is trained to predict the next word in a sentence, given the context of the previous words. During training, the model learns to understand the meaning of words and their relationships with each other, allowing it to generate coherent and fluent text.\\n\\nGPT can be fine-tuned on a wide range of NLP tasks such as text classification, text generation, text summarization, question answering, etc. The pre-training process allows the model to learn a general understanding of the language, allowing it to be adapted to a specific task with just a small amount of task-specific training data.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='Additionally, GPT-3(Chat GPT), a more advanced version of GPT-2, has been developed and with its even more impressive performance on a wide range of NLP tasks. it has the ability to perform well in various NLP tasks without fine-tuning, showing the potential for GPT models to become general-purpose NLP models.\\n\\nGPT-3 (Chat GPT) is a powerful language model that can be used in a wide range of NLP tasks, including chatbot systems, with its ability to generate natural-sounding text based on user input, it could make chatbot systems more human-like and engaging for users.\\n\\n\\nn summary, GPT is a pre-trained model that uses a transformer-based architecture to generate natural language text and it has shown impressive performance on various NLP tasks.\\n\\nWord2Vec use cases', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='Word2Vec use cases\\n\\nWord2Vec is a word embedding algorithm that is used to map words or phrases from a vocabulary to a continuous vector space. It is a powerful tool for natural language processing (NLP) tasks, and has many practical use cases. Some of the most common use cases for Word2Vec are:', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}), Document(page_content='Text classification: Word2Vec can be used to convert text into numerical data, which can then be used as input for a machine learning model to perform text classification tasks such as sentiment analysis or spam detection.\\nWord similarity: One of the main advantages of Word2Vec is that it can learn the similarity of words, it can be used to find synonyms or related words based on the similarity of the learned embeddings.\\nNamed Entity Recognition: The embeddings can be used as features for a model that identifies entities in text such as organizations, person and locations\\nText Generation: The embeddings learned by Word2Vec can be used as input to a language model such as LSTM, to generate text that is similar to the training data.\\nClustering: The embeddings can be used to cluster words or documents into groups, based on their similarity.\\nDimensionality reduction: Word2Vec can be used to reduce the dimensionality of the text data, which can be useful for visualizing the relationships between words or for reducing the computational cost of other NLP tasks.\\nRecommender systems: The embeddings can be used to find similar items (such as products, movies, or articles) based on their similarity in word usage, which can be useful for building recommenders systems\\nMachine Translation: Word2Vec can be used as a pre-processing step for machine translation, where the embeddings of the words in the source language can be used to initialize the embeddings of the words in the target language\\nThese are just a few examples of the many potential use cases for Word2Vec, and it’s likely that new applications will continue to be discovered as the field of NLP evolves.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'})]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of text :\",len(texts))  # decresing the chunk size increases the length of text\n",
    "print(type(texts))\n",
    "print(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
    "\n",
    "# In case we are using hugging face embeddings \n",
    "# hface_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDIS\n",
    "\n",
    "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
    "REDIS_PORT = os.getenv(\"REDIS_PORT\", 6379)\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"sam2307\") #1h5tBoYxZbmX1oAdSCKZxxGsfIQ81kF9\n",
    "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\"\n",
    "os.environ['REDIS_URL'] = REDIS_URL\n",
    "\n",
    "# docsearch = RedisVectorStore.from_existing_index(embedding=openai_embeddings,index_name='medium',redis_url=REDIS_URL)\n",
    "# text_vector = RedisVectorStore.from_existing_index(texts=texts,embedding=openai_embeddings,index_name='medium')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS\n",
    "\n",
    "Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.\n",
    "\n",
    "- Faiss is written in C++ with complete wrappers for Python. Some of the most useful algorithms are implemented on the GPU. It is developed by Facebook AI Research.\n",
    "- supports GPU arch as well .\n",
    "- Based on L2 Norm and Knearest Neighbours to get argmin of vectors\n",
    "\n",
    "[More details of FAISS](https://faiss.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_documents(documents=texts,embedding=openai_embeddings)\n",
    "docsearch.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samartht/anaconda3/envs/pylangchain/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:201: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(),chain_type='stuff',vectorstore=docsearch,return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is a word embedding? Give me answer in 15-20 words as a beginner.'\n",
    "result = qa({'query':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:what is a word embedding? Give me answer in 15-20 words as a beginner.\n",
      "Result: A word embedding is a technique used in natural language processing to map words or phrases from a vocabulary to a continuous vector space, allowing similar words to have similar representations and dissimilar words to have dissimilar representations.\n"
     ]
    }
   ],
   "source": [
    "print(\"Query:{}\".format(result['query']))\n",
    "print(\"Result:{}\".format(result['result']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look source documents fromm where the query has been searched and LLM processed it for the result generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Word embedding is a technique used in natural language processing (NLP) to map words or phrases from a vocabulary to a continuous vector space. The goal of word embedding is to capture the meaning of a word in a vector representation, such that similar words have similar representations and dissimilar words have dissimilar representations. There are several word embedding techniques, but the most popular ones are:', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}),\n",
       " Document(page_content='A word embedding is a representation of a word as a vector of numeric values.\\n\\nFor example, the word “night” might be represented as (-0.076, 0.031, -0.024, 0.022, 0.035).\\n\\nIt means that if there are two words used similarly in the text, they will have the same vector representations.\\n\\nAfter mapping words into the vector space, we can use the vector math to find words with similar semantics.\\n\\nWe can either create a custom word embedding based on our project or use pre-built word embeddings by loading them.\\n\\nTo find the word embedding of text, first we need to split it to sentence and then tokenize the sentences.\\n•Then we import Word2Vec model from genism and train it on our text.\\n\\n•There are many parameters that we can specify for our Word2Vec model.\\n\\n“size” is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\\n\\nBigger size values require more training data, but can lead to better (more accurate) models.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}),\n",
       " Document(page_content='The main difference between these two algorithms is that the CBOW algorithm is faster and the Skip-Gram algorithm is more accurate. The choice of algorithm will depend on the specific requirements of your project.\\n\\nNeural word embedding\\n\\nNeural word embeddings are a type of word embedding technique that uses neural networks to learn the vector representations of words. These vector representations, also known as embeddings, capture the meaning of a word in a continuous vector space, such that similar words have similar representations and dissimilar words have dissimilar representations.\\n\\nStatic word embedding', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'}),\n",
       " Document(page_content='Word2Vec: It is a neural network-based technique that uses a shallow neural network to learn the vector representations of words. Word2Vec has two algorithms: Continuous Bag of Words (CBOW) and Skip-Gram.\\nGloVe: This method stands for “Global Vectors for Word Representation” It combines the advantages of the matrix factorization techniques and the global corpus statistics.\\nFastText: It is an extension of the word2vec model that learns to take into account subword information (i.e. n-grams) of a word in addition to the word itself. It’s particularly useful for handling rare or out-of-vocabulary words.\\nELMO: This is deep contextualized word representations which is pre-trained models which are task-agnostic, giving it more context while learning word representation.\\nBERT: Like ELMO, BERT is also a deep contextualized word representation. BERT is trained on a massive amount of data and can be fine-tuned on a variety of tasks.\\nThese word embedding techniques are widely used in various natural language processing tasks, such as text classification, text generation, machine translation, named entity recognition, and more.', metadata={'source': '../NLP-Basic2Advanced/data/medium_logs_1.txt'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['source_documents']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
