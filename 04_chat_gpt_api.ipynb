{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of models available :68\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "print(\"Total number of models available :{}\".format(len(openai.Model.list()['data'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage | davinci | babbage-code-search-code | text-similarity-babbage-001 | text-davinci-001 | ada | curie-instruct-beta | babbage-code-search-text | babbage-similarity | gpt-3.5-turbo | code-search-babbage-text-001 | gpt-3.5-turbo-0301 | code-davinci-002 | code-cushman-001 | code-search-babbage-code-001 | text-ada-001 | text-embedding-ada-002 | text-similarity-ada-001 | text-davinci-insert-002 | ada-code-search-code | ada-similarity | whisper-1 | text-davinci-003 | code-search-ada-text-001 | text-search-ada-query-001 | text-curie-001 | text-davinci-edit-001 | davinci-search-document | ada-code-search-text | text-search-ada-doc-001 | code-davinci-edit-001 | davinci-instruct-beta | text-similarity-curie-001 | code-search-ada-code-001 | ada-search-query | text-search-davinci-query-001 | curie-search-query | davinci-search-query | text-davinci-insert-001 | babbage-search-document | ada-search-document | text-search-curie-query-001 | text-search-babbage-doc-001 | text-davinci-002 | curie-search-document | text-search-curie-doc-001 | babbage-search-query | text-babbage-001 | text-search-davinci-doc-001 | text-search-babbage-query-001 | curie-similarity | curie | text-similarity-davinci-001 | davinci-similarity | cushman:2020-05-03 | ada:2020-05-03 | babbage:2020-05-03 | curie:2020-05-03 | davinci:2020-05-03 | if-davinci-v2 | if-curie-v2 | if-davinci:3.0.0 | davinci-if:3.0.0 | davinci-instruct-beta:2.0.0 | text-ada:001 | text-davinci:001 | text-curie:001 | text-babbage:001 | <built-in method format of str object at 0x7f4b02528270>\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for model_name in openai.Model.list()['data']:\n",
    "    models.append(model_name['id'])\n",
    "    print(model_name['id'],end=\" | \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Model ['whisper-1']\n",
      "text models ['text-similarity-babbage-001', 'text-davinci-001', 'babbage-code-search-text', 'code-search-babbage-text-001', 'text-ada-001', 'text-embedding-ada-002', 'text-similarity-ada-001', 'text-davinci-insert-002', 'text-davinci-003', 'code-search-ada-text-001', 'text-search-ada-query-001', 'text-curie-001', 'text-davinci-edit-001', 'ada-code-search-text', 'text-search-ada-doc-001', 'text-similarity-curie-001', 'text-search-davinci-query-001', 'text-davinci-insert-001', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'text-davinci-002', 'text-search-curie-doc-001', 'text-babbage-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'text-similarity-davinci-001', 'text-ada:001', 'text-davinci:001', 'text-curie:001', 'text-babbage:001']\n",
      "Chat models ['gpt-3.5-turbo', 'gpt-3.5-turbo-0301']\n"
     ]
    }
   ],
   "source": [
    "audio_model = [model_name for model_name in models if model_name.startswith('whisper')]\n",
    "text_model = [model_name for model_name in models if 'text' in model_name]\n",
    "chat_model = [model_name for model_name in models if model_name.startswith(\"gpt\")]\n",
    "print(\"Audio Model {}\".format(audio_model))\n",
    "print(\"text models {}\".format(text_model))\n",
    "print(\"Chat models {}\".format(chat_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ChatGPT models \n",
    "* Model endpoint compatibility\n",
    "- ENDPOINT\tMODEL NAME\t\n",
    "- /v1/chat/completions\tgpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-0301\t\n",
    "- /v1/completions\ttext-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001, davinci, curie, babbage, ada\t\n",
    "- /v1/edits text-davinci-edit-001\ttext-davinci-edit-001, code-davinci-edit-001\t\n",
    "- /v1/audio/transcriptions\twhisper-1\t\n",
    "- /v1/audio/translations\twhisper-1\t\n",
    "- /v1/fine-tunes\tdavinci, curie, babbage, ada\t\n",
    "- /v1/embeddings\ttext-embedding-ada-002, text-search-ada-doc-001\t\n",
    "- /v1/moderations\ttext-moderation-stable, text-moderation-latest\n",
    "\n",
    "### GPT-3.5-turbo \n",
    "\n",
    "* Keep an eye on the tokens , can not be more than 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{'role':\"user\",\"content\":\"Will AI is going to dominate the word\"}])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat GPT Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reply_content(msg):\n",
    "    reply_list = []\n",
    "    for reply in completion['choices']:\n",
    "        reply_list.append(reply['message']['content'].strip())\n",
    "    return reply_list\n",
    "\n",
    "def get_total_tokens(msg):\n",
    "    return completion['usage']['total_tokens']\n",
    "\n",
    "\n",
    "def get_completion_tokens(msg):\n",
    "    return completion['usage']['completion_tokens']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replies ['As an AI language model, I cannot predict the future with certainty. However, it is unlikely that AI will dominate the world completely. AI has its own limitations and requires human intervention to operate effectively. While AI is already being used in various industries, it cannot replace human creativity and emotions. It is essential to balance the benefits of AI technology with ethical considerations and ensure that it is used for the common good. Hence, it is not a question of AI dominating the world, but rather human beings working together with AI to create a better future.']\n",
      "Comlpetion Tokens 110\n",
      "Tokens Used : 15\n"
     ]
    }
   ],
   "source": [
    "reply_content = get_reply_content(completion)\n",
    "print(\"Replies {}\".format(reply_content))\n",
    "print(\"Comlpetion Tokens {}\".format(get_completion_tokens(completion)))\n",
    "print(\"Tokens Used : {}\".format(get_token_used(completion)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History - Mangage own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input was: How to learn NLP with hugging face\n"
     ]
    }
   ],
   "source": [
    "msg_history = []\n",
    "user_input = input(\">:\")\n",
    "\n",
    "print(\"User input was: {}\".format(user_input))\n",
    "msg_history.append({\"role\":\"user\",\"content\":user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=msg_history)\n",
    "\n",
    "msg_history.append({\"role\":\"assistant\",\"content\": get_reply_content(completion)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Completion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txtcomp(rawtxt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"assistant\", \"content\": \"Hello on Chat GPT\"},\n",
    "            {\"role\": \"user\", \"content\": rawtxt},\n",
    "        ]\n",
    "            )\n",
    "    result = ''\n",
    "    for choice in response.choices:\n",
    "        result += choice.message.content\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\">:\")\n",
    "print(txtcomp(user_input))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization\n",
    "#### Text-Davinci-003\n",
    "* Model  : Can do any language task with better quality, longer output, and consistent instruction-following than the curie, babbage, or ada models. Also supports inserting completions within text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "We are trying to create a production-ready pipeline, but it is taking longer than expected. We are testing our model on Point-of-Interest signals and are looking at other signals like GPS and street images to improve its accuracy in the USA and UK.\n"
     ]
    }
   ],
   "source": [
    "def summpred(rawtxt):\n",
    "    text_sum = \"Summarize this for 9th grade student:\\n\\n\"+rawtxt\n",
    "    completion = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=text_sum,\n",
    "    temperature=0.7,  ## Generate randomness in the output\n",
    "    max_tokens = 64,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0\n",
    "    )   \n",
    "\n",
    "    response = completion.choices[0].text\n",
    "    return response\n",
    "\n",
    "test_text = \"We are stuck to create a production ready pipeline because of overture delayed timelines for MSFT building footprints.So for now we are going to evaluate our model how it performs on POI signals and will keep venturing on other signals like GPS and street imagenary to improve APA score for USA and GBR countries\"\n",
    "print(summpred(test_text))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat GPT Audio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_media_path = \"/mnt/c/Users/medha/Downloads/Video/testing4_audio.mp4\"\n",
    "test_media = open(test_media_path,'rb')\n",
    "\n",
    "response = openai.Audio.transcribe(api_key='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',model='whisper-1',file=test_media,prompt='')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
